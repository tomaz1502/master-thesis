In this chapter we describe our main contribution, which is a module
for lifting the scripts presented in Section~\ref{sec:gen-scripts} into
Lean proofs, based on the certifying transformations approach.
%
The specific manner in which we applied
this concept in our context was to develop a set of tactics matching
the rules present in cvc5's proof calculus.
A cvc5 proof is then a sequence of applications of those tactics.
When we ask Lean's kernel to check the proof script, it will execute the tactics
one by one. Each one of them inspects the current context
and produces a proof term corresponding to a proof of a specific
case of the rule represented by that tactic.
The checker then verifies the correctness of each proof, closing the original goal only
if all checks were succesful.

There are two options for implementing these tactics: the first one is to generate
a value of type \texttt{Syntax} (that is, a piece of Lean's code) that proves
the theorem in question and invoke \texttt{evalTactic} using this value;
the second option is to craft an \texttt{Expr} that corresponds to a value
of the appropriate type. In general,
it is simpler to generate \texttt{Syntax}, as the elaborator will fill in
some details for us, such as the implicit arguments for functions. On the
other hand, we can skip calls to the elaborator if we produce an \texttt{Expr}
instead, which will potentially lead to a gain in performance. With this in mind,
we decided to employ the second option.


Another important difference from the certified transformations approach is that we do not use the \texttt{term}
type anymore. The main reason for mapping MSFOL formulas to this inductive type
instead of native Lean expressions is the flexibility achieved by this representation.
While it is straightforward to define a function that inspects and manipulates the structure of
a \texttt{term} by pattern matching, there is no way to do the same for certain Lean types (\texttt{Prop}, for
instance) without recurring to metaprogramming. As we have shown in Section~\ref{sec:metaLean},
the metaprogramming context grants us access to the internal representation of any expression
through the \texttt{Expr} type, which can be inspected and manipulated in the same way
as \texttt{term}. Since the framework for writing tactics is based on metaprogramming, we
do not need to rely on the flexibility of the \texttt{term} type. Therefore, we
have decided to not use the deep embedding anymore, and translate MSFOL formulas directly
to Lean expressions.

The snippet in Figure~\ref{notModusPonens2} shows an example of proof in the new format.
It corresponds to the proof that cvc5 produces to the SMT-Lib query presented
in Figure~\ref{negModusPonens}. A considerable portion of the rules (including
every rule regarding CNF transformation) can be easily proved using classical
reasoning. Instead of mapping such rules to tactics, we just proved them as theorems.
Those theorems will not be presented here. For a complete overview of the rules, their
statement and whether they were implemented with a tactic or a
theorem, refer to Table~\ref{tab:rules}.
All the rules present in the proof in Figure~\ref{notModusPonens2} were mapped to
theorems except for resolution (line 8), which was mapped to a tactic that we will
present.


\begin{figure}[t]
\begin{minted}[linenos]{lean}
  theorem cvc5_th0 {P Q : Prop} : (Not (P → ((P → Q) → Q))) → False :=
    fun lean_a0 : (Not (P → ((P → Q) → Q))) => by
      have lean_s0 : (Not ((P → Q) → Q)) := notImplies2 lean_a0
      have lean_s1 : (P → Q)             := notImplies1 lean_s0
      have lean_s2 : (Or (Not P) Q)      := impliesElim lean_s1
      have lean_s3 : P                   := notImplies1 lean_a0
      have lean_s4 : Q                   :=
        by R2 lean_s2, lean_s3, P, [1, 0]
      have lean_s5 : (Not ((P → Q) → Q)) := notImplies2 lean_a0
      have lean_s6 : (Not Q)             := notImplies2 lean_s5
      exact (show False from contradiction lean_s4 lean_s6)
\end{minted}
\caption{Proof script using certifying transformations.}\label{notModusPonens2}
\end{figure}

\paragraph{Representation of clauses.} Internally, clauses are represented by cvc5 as
a lists of terms. A list of terms can correspond to many distinct
clauses, depending on how you parenthesize them. Implicitly, cvc5 is using the
fact that disjunction is associative, which implies that all these clauses
are equivalent, therefore they do not need to be differentiated inside the calculus.
When these lists of terms are sent to Lean, we are forced to chose a way to parenthesize
those terms and build our proofs taking into account this format.
We chose to parenthesize them in a right-associative way.
Therefore, if cvc5 is
representing some clause as the list $[t_{1}, t_{2}, t_{3}, t_{4}]$ we will represent it in Lean as
\texttt{t₁' ∨ (t₂' ∨ (t₃' ∨ t₄'))}, where \texttt{tᵢ'} is the representation of $t_{i}$.
Since the built-in operator \texttt{∨} is right-associative, we can omit the
parenthesis. Throughout the implementation of the tactics, we always assume
that a clause that is received as premise comes parenthesized in this way,
and every clause that is proved by a tactic also has this format.

In the rest of this chapter we will give an overview of the implementation of some of the
tactics. We present their statements with the same format used in cvc5's
documentation\footnote{The documentation of cvc5's rules can be found at:
  \url{https://cvc5.github.io/docs/cvc5-1.0.2/proofs/proof_rules.html}}, that is,
for a tactic that has a conclusion $\psi$, premisses $\psi_{1}, \cdots, \psi_{n}$ and
parameters $t_{1}, \cdots, t_{n}$, we write:
\[
  \infer[]{\psi}{\psi_{1}, \cdots, \psi_{n} \mid t_{1}, \cdots, t_{n}}
\]

\section{Auxiliary Tactics}

This first class of tactics does not correspond to cvc5's rules. They were implemented
to facilitate the implementation of the main tactics.



\subsection{Parenthesizing prefixes of clauses}

Given a number \texttt{i} and a proof \texttt{pf}
of the validity of a clause, the application \texttt{groupClausePrefix pf, i} proves the same clause, with the
first $i$ propositions parenthesized. The disjunction of the first $i$ propositions is a single element of the output clause.  The precise statement of this rule
is the following:

\[
  \infer[]{(P_{1} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}}
    {P_{1} \vee \cdots \vee P_{n} \mid i}
\]

This tactic employs the following two theorems, which were easily proven:

\begin{itemize}
  \item \mintinline{lean}{orAssoc {A B C : Prop} : A ∨ B ∨ C → (A ∨ B) ∨ C}
  \item \mintinline{lean}{congOrLeft {A B C : Prop} (hyp : A → B) : C ∨ A → C ∨ B}
\end{itemize}

% Also, it will make use of the \texttt{apply} tactic. If the current goal is \texttt{Q}, and
% we have a proof \texttt{h} of \texttt{P → Q}, we can reduce the goal to \texttt{P} with
% \texttt{apply h}.

% Let's instantiate, in the theorem \texttt{orAssoc}, \texttt{A} to $P_{1}$, \texttt{B}
% to $P_{2} \vee \cdots \vee P_{i}$ and \texttt{C} to $P_{i + 1} \vee \cdots \vee P_{n}$. This will yield a proof of
% of $P_{1} \vee (P_{2} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n} \rightarrow (P_{1} \vee \dots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$. Since
% the proof script is annotated with the type it expects from each tactic application,
% we can assume that the current goal is $(P_{1} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$. Therefore, we can change the goal to $P_{1} \vee (P_{2} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$ with \texttt{apply orAssoc} (Lean's kernel can infer what are the implicit arguments in this case). Now we would like to apply \texttt{orAssoc} again to remove $P_{2}$ from the parenthesized group. We cannot do it directly, due to the term $P_{1}$ to the left of the parenthesis. We solved this
% problem by using \texttt{congOrLeft}, with \texttt{C} instantiated to $P_{1}$ and
% \texttt{hyp} instantiated to \texttt{orAssoc}, which yields a proof of the correct
% statement. Therefore, we can change the goal to $P_{1} \vee P_{2} \vee (P_{3} \vee \cdots \vee P_{i}) \vee \cdots \vee P_{n}$ with \texttt{apply (congOrLeft orAssoc)}.

First, let's build a proof term for $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$. We instantiate, in \texttt{orAssoc}, the parameter \texttt{A} to $P_{i - 1}$, \texttt{B} to $P_{i}$ and \texttt{C} to $P_{i + 1} \vee \cdots \vee P_{n}$, obtaining the term:

\begin{center}
    $f_{1}: P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n}   \rightarrow (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n} $
\end{center}

Then, we apply \texttt{congOrLeft} to $f_{1}$ instantiating \texttt{C} to $P_{i - 2}$, which produces:

\begin{center}
    $f_{2}: P_{i - 2} \vee P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n}   \rightarrow P_{i - 2} \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n} $
\end{center}

We repeat this process until we get the term $f_{i - 1}$ of type $P_{1} \vee \cdots \vee P_{n} \rightarrow P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$. Applying $f_{i - 1}$ to the original hypothesis \texttt{h} yields a term \texttt{h'} with type $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.

A clause is composed by any kind of propositions, including disjunctions.
Therefore, the term $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$ is a new clause
with $n - 1$ propositions, in which the $(i - 1)$-th proposition is the disjunction
$P_{i - 1} \vee P_{i}$. This means that our original problem can be reduced to
group the first $i - 1$ terms of this clause.
With this in mind, we repeat the process of composing \texttt{congOrLeft} with
\texttt{orAssoc} and apply the result to \texttt{pf'}, obtaining
a new term of type $P_{1} \vee \cdots \vee (P_{i - 2} \vee P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.
We keep repeating this process until the whole prefix is grouped.


\begin{figure}[t]
\begin{minted}[linenos]{lean}
def groupPrefixCore (pf : Expr) (i : Nat) : MetaM Expr := do
  let clause  : Expr      ← inferType pf
  let props   : List Expr ← collectPropsInClause clause
  if i > 0 && i < List.length props then
    let lemmas : List Expr ← groupPrefixLemmas props (i - 1)
    let answer : Expr :=
      List.foldl (fun acc lem => Expr.app lem acc) pf lemmas
    return answer
  else throwError
    "[groupClausePrefix]: invalid prefix length"
\end{minted}
\caption{Implementation of the tactic GroupClausePrefix}\label{groupClause}
\end{figure}

The snippet in Figure~\ref{groupClause} and shows the
implementation of the core functionality of this tactic.
The parameters \texttt{pf} and \texttt{i} of the function \texttt{groupPrefixCore}
represent, respectively, the proof of the validity of the original clause
and the length of the prefix that should be grouped. In line 2, we obtain
an \texttt{Expr} corresponding to the original clause by inspecting the type
of \texttt{pf}, with the built-in routine \texttt{inferType}. Then, in line 3,
we use the function \texttt{collectPropsInClause} (defined by us) to
extract a \texttt{List} with each one of the propositions in the clause.
Next, we check if the prefix length is valid. If it is, we apply our function
\texttt{groupPrefixLemmas} to obtain a list of expressions. Each element in this list
is a composition of \texttt{congOrLeft} and \texttt{orAssoc} described before.
One important difficulty in the implementation of this function is that implicit parameters cannot always be automatically computed in the \texttt{Expr} level, so
this function also computes some of the implicit arguments that have to be
passed to \texttt{congOrLeft} and \texttt{orAssoc}. Lean has a built-in
functionality to automatically infer part of those arguments, but a significant
amount of them have to be manually constructed.
Finally, we use \texttt{foldl}
in line 7 to apply each one of the lemmas to \texttt{pf}, accumulating the results.

In order to use \texttt{groupPrefixCore} as a tactic, we implement
a function of type \texttt{Syntax → TacticM Unit}, which parses the \texttt{Syntax}
to obtain \texttt{pf} and \texttt{i} and use the \texttt{Expr} generated by
\texttt{groupPrefixCore} to close the current goal.

The following snippet shows an example of usage of this tactic:

\newpage

\begin{minted}{lean}
  theorem group3 : A ∨ B ∨ C ∨ D ∨ E → (A ∨ B ∨ C) ∨ D ∨ E := by
    intro h
    groupClausePrefix h, 3
\end{minted}

We can use the \texttt{\#print} command to inspect the proof term generated by the tactic, and
verify that it is, indeed, the one we described:

\begin{minted}{lean}
  #print group3 -- fun {A B C D E} h => orAssoc (congOrLeft orAssoc h)
\end{minted}
% \tom{proof term size is $\mathcal{O}(i^{2})$. Maybe say a few words about this?}

\subsection{Moving terms inside clauses}

Given a proof \texttt{pf} of the validity of a clause and an index \texttt{i} of a proposition to be moved, the \texttt{pull} tactic should produce a proof term for the same clause,
with $i$-th term moved to the first position. This tactic also facilitates the implementation
of the other tactics and does not exist in cvc5's proof calculus. The precise
statement of this rule is the following:

\[
  \infer[]{P_{i} \vee P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n}}{P_{1} \vee \cdots \vee P_{n}  \mid i, s}
\]


We need three theorems to implement \texttt{pull}, which are easy to prove:

\begin{itemize}
  \item \mintinline{lean}{orComm {A B C : Prop} : A ∨ B → B ∨ A}
  \item \mintinline{lean}{congOrRight {A B C : Prop} (hyp : A → B) : A ∨ C → B ∨ C}
  \item \mintinline{lean}{orAssocConv {A B C : Prop} : (A ∨ B) ∨ C → A ∨ B ∨ C}
\end{itemize}

To build the required proof term, let's first apply our tactic \texttt{groupClausePrefix} at \texttt{pf}, grouping the prefix of length $i - 1$.
We cannot use the tactic itself, as we are working on the context of \texttt{MetaM}, but we can invoke directly the function \texttt{groupPrefixCore}
to produce the required term. We will obtain the following term:

\begin{center}
  $pf_{1} : (P_{1} \vee \cdots \vee P_{i - 1}) \vee P_{i} \vee \cdots \vee P_{n}$
\end{center}

Then, we use again our function \texttt{groupPrefixCore} in \texttt{pf₁}, grouping the prefix of length 2. This provides the term:

\begin{center}
  $pf_{2} : ((P_{1} \vee \cdots \vee P_{i - 1}) \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$
\end{center}

Next, we will define a new term \texttt{pf₃} as \texttt{congOrRight orComm pf₂}. Given the appropriate instantiation for the implicit parameters,
this application flips the position of $P_{1} \vee \cdots \vee P_{i - 1}$ and $P_{i}$, while not modifying the suffix $P_{i + 1} \vee \cdots \vee P_{n}$.
Therefore, \texttt{pf₃} has type $(P_{i} \vee P_{1} \vee \cdots \vee P_{i - 1}) \vee P_{i + 1} \vee \cdots \vee P_{n}$ (we do not need to parenthesize
$P_{1} \vee \cdots \vee P_{i - 1}$ as $\vee$ is right-associative). Finally we use another tactic we implemented, \texttt{ungroupClausePrefix},
to remove the parenthesis around $P_{i} \vee P_{1} \vee \cdots \vee P_{i - 1}$, concluding our goal.
This tactic is similar to \texttt{groupClausePrefix}. The
difference is that it substitutes the theorem \texttt{orAssoc} by \texttt{orAssocConv} and it applies the theorems in reverse order, i.e.\ it
starts with an application of \texttt{orAssocConv}, then it applies \texttt{congOrLeft orAssocConv} and so on.

\paragraph{Disambiguating clauses.} The difference between the representation of
clauses in cvc5 and in Lean leads to an ambiguity in applications of this tactic.
While there is no distinction in Lean between, for instance, the clauses $A \vee B \vee C \vee D \vee E$
and $A \vee B \vee C \vee (D \vee E)$, this distinction can be made inside the SMT solver. Indeed, if
we have a term \texttt{pf} in Lean of type $A \vee B \vee C \vee D \vee E$ and cvc5 prints the tactic
application \texttt{pull pf, 4}, there is no way to know, inside Lean, if it is expecting to receive
a proof of $D \vee A \vee B \vee C \vee E$ or $(D \vee E) \vee A \vee B \vee C$. To address this issue,
we adapted cvc5 to print an extra parameter \texttt{s} to this tactic (and to others that suffered from the same problem),
which is a natural number corresponding to the index of the last proposition in the clause from the
point of view of the SMT solver. Therefore, for the example we just described, it would print
\texttt{pull pf, 4, 4} to indicate that it wants a proof for $(D \vee E) \vee A \vee B \vee C$ and
\texttt{pull pf, 4, 5} to indicate that it wants a proof for $E \vee A \vee B \vee C \vee D$.

\section{Boolean reasoning}

We now present the tactics regarding Boolean reasoning. One of them corresponds to
the resolution rule, which as we have shown in Section~\ref{sec:pcBool}, is the main
building block for proofs in Propositional Logic. The other two, \textit{factor} and
\textit{permutateClause}, are used to justify implicit steps that SAT solvers
perform while solving formulas through resolution.

\subsection{Reordering clauses}

Given the proof \texttt{pf} of a clause, a permutation \texttt{perm} and the index \texttt{s} of the last
proposition in the clause, the application \texttt{permutateClause pf, perm, s} provides a proof for the same clause, with
the propositions permutated according to \texttt{perm}.
This is the first tactic that matches a rule used by cvc5\footnote{The documentation of this rule can be found at \url{https://cvc5.github.io/docs/cvc5-1.0.2/proofs/proof\_rules.html\#\_CPPv4N4cvc58internal6PfRule10REORDERINGE}}. The precise statement
of this rule is the following:

\[
  \infer[]{P_{perm(1)} \vee \cdots \vee P_{perm(n)}}{P_{1} \vee \cdots \vee P_{n} \mid perm, s}
\]


Since we have \texttt{pull}, we can write a tactic matching
this rule using a relatively straightforward approach. We simply iterate through the permutation in
reverse order, and, for each index \texttt{i} we go through, we run the \texttt{pull} tactic
with \texttt{i} as the argument. With this strategy, the last term we will bring to the first position is
$P_{perm(1)}$, therefore, the final clause we will produce will have the correct element in the first position.
Similarly, the second to last element we will bring to the first
position is $P_{perm(2)}$. After pulling it, we will bring another term to the first position ($P_{perm(1)}$), which will make $P_{perm(2)}$
end up in the second position, as required. By extending this reasoning to all the terms in the clause,
we can conclude that each one of them will be placed in the right position. Notice that the index \texttt{s} of
the last proposition does \textit{not} change during this process, so we can always use \texttt{s} as the argument
required by \texttt{pull}.

\begin{figure}[t]
\begin{minted}[linenos]{lean}
  def pullProps (props : List Expr) (acc : Expr) (s : Nat) :
      MetaM Expr :=
    match props with
      | [] => return acc
      | e::es => do
          let pulled ← pullCore e acc s
          pullProps es pulled s

  def permutateClauseCore (pf : Expr) (perm : List Nat)
      (s : Nat) : MetaM Expr := do
    let clause : Expr     ← inferType pf
    let props : List Expr ← collectPropsInClause' clause s
    let permutatedProps   := permutateList props (List.reverse perm)
    pullProps permutatedProps pf s
\end{minted}
\caption{Implementation of the tactic permutateClause.}\label{permClauseImp}
\end{figure}

The code in Figure~\ref{permClauseImp} shows our implementation of this tactic.
The main function is \texttt{permutateClauseCore}. It starts by obtaining a list of
\texttt{Expr} corresponding to the terms in the clause using \texttt{inferType}
and \texttt{collectPropsInClause} (lines 11 and 12), in the same manner as
\texttt{groupPrefixCore}. Next, in line 13, it permutates this list according
to the reverse of the input permutation. Finally, in line 14, it invokes
\texttt{pullProps} with the permutated list, \texttt{pf} and \texttt{s}.
This auxiliary method traverses the input list, applying \texttt{pull} to
each \texttt{Expr} it encounters, accumulating the result.

The following snippet shows an example of usage of this tactic:

\begin{minted}{lean}
  theorem perm1 :
      A ∨ B ∨ C ∨ (D ∨ E ∨ F) → (D ∨ E ∨ F) ∨ B ∨ C ∨ A := by
    intro h
    permutateClause h, [3, 1, 2, 0], 3
\end{minted}



\subsection{Resolution}

Given two proofs of clauses, \texttt{pfP} and \texttt{pfQ}, a proposition \texttt{A}
(also known as the \textit{pivot}) such that
\texttt{A} is an element of the first clause and \texttt{¬ A} is an element of the second clause and the indices \texttt{s₁} and \texttt{s₂} of the last propositions in each clause, this tactic produces a proof for the clause composed by the concatenation
of the two, removing the pivot from the first and the negation of the pivot from the second. If there are multiple instances of the pivot in the first clause, it removes
the first one. Similarly, if there are multiple instances of the negation of the pivot
in the second clause, it also removes the first one. The precise statement
is the following:

\[
  \infer[]{P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n} \vee Q_{1} \vee \cdots \vee Q_{j - 1} \vee Q_{j + 1} \vee \cdots \vee Q_{m}}{P_{1} \vee \cdots \vee P_{i - 1} \vee A \vee P_{i + 1} \vee \cdots \vee P_{n}, Q_{1} \vee \cdots \vee Q_{j - 1} \vee \neg A \vee Q_{j + 1} \vee \cdots \vee Q_{m}, \mid A, s_{1}, s_{2}}
\]

We will need four theorems to implement the resolution tactic, which are all easy to prove:

\begin{itemize}
  \item \mintinline{lean}{resolutionSpecialCase1 {A B C : Prop} : A ∨ B → ¬ A ∨ C → B ∨ C}
  \item \mintinline{lean}{resolutionSpecialCase2 {A B : Prop} : A ∨ B → ¬ A → B}
  \item \mintinline{lean}{resolutionSpecialCase3 {A C : Prop} : A → ¬ A ∨ C → C}
  \item \mintinline{lean}{resolutionSpecialCase4 {A : Prop} : A → ¬ A → False}
\end{itemize}

First, we apply \texttt{pull} to \texttt{pfP} to bring the first occurence of \texttt{A} to the first position. This will result
in a proof \texttt{pfP'} of the clause:

\begin{center}
  $A \vee P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n}$
\end{center}

Next, we do the same at \texttt{pfQ}, bringing the negation of the pivot to the first position. We will obtain a proof \texttt{pfQ'} of
the clause:

\begin{center}
  $\neg A \vee Q_{1} \vee \cdots \vee Q_{j - 1} \vee Q_{j + 1} \vee \cdots \vee Q_{m}$
\end{center}

Now, we apply one of the \texttt{resolutionSpecialCase} theorems to \texttt{pfP'} and \texttt{pfQ'}. We decide which theorem
to apply based on whether or not each clause has other terms apart from the pivot. If both clauses have other terms (i.e. $n > 0$ and
$m > 0$) we apply \texttt{resolutionSpecialCase1}, instantiating \texttt{B} to $P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n}$ and \texttt{C} to $Q_{1} \vee \cdots \vee Q_{j - 1} \vee Q_{j + 1} \vee \cdots \vee Q_{m}$. If the second clause consists of only the negation of the pivot and the first one has other terms, we apply \texttt{resolutionSpecialCase2}. If the first clause consists of only the pivot and the second one has other terms, we apply \texttt{resolutionSpecialCase3}. If both clauses consist of a single element, we apply \texttt{resolutionSpecialCase4}. In this case, the tactic will produce a proof of \texttt{False}, which is the way we represent the empty clause.

Notice that, if we apply \texttt{resolutionSpecialCase1} and the first clause had more than 1 element
apart from the pivot, the term we get is not the required one. Its type will have the
propositions from the first clause parenthesized, i.e.\ it will have the following form:

\begin{center}
  $(P_{1} \vee \cdots \vee P_{n}) \vee Q_{1} \vee \cdots \vee Q_{m}$
\end{center}

We fix this issue with the tactic \texttt{ungroupClausePrefix}, which was mentioned before.

The original rule used by cvc5 has another
boolean parameter, \textit{pol}. If pol is set to true, then the semantics of the rule matches
exactly the tactic we described. If pol is set to false, the rule expects to find the pivot
negated in the first clause and not negated in the second. We have implemented a separate
tactic for this case. The function that implements the core functionality of both tactics is
the same.

The following snippet shows one example of usage of each tactic. \texttt{R1} corresponds to
the rule with pol set to true, and \texttt{R2} corresponds to the rule with pol set to false.
The last parameter of the tactic is a list with two elements, corresponding to the indices
of the last propositions in each clause.

\begin{minted}{lean}
  theorem res1 : A ∨ B ∨ C ∨ D → E ∨ ¬ B → A ∨ (C ∨ D) ∨ E := by
    intros h₁ h₂
    R1 h₁, h₂, B, [2, 1]

  theorem res2 : ¬ A → B ∨ A ∨ C → B ∨ C := by
    intros h₁ h₂
    R2 h₁, h₂, A, [0, 2]
\end{minted}

\subsection{Removing duplicates}

Given a proof \texttt{pf} of a clause and the index \texttt{s} of the last proposition in the clause,
the \textit{factor} tactic produces a proof of the same clause, with all duplicated literals of it removed. The precise statement of this rule is the following:

\[
  \infer[]{removeDuplicates(P_{1} \vee \cdots \vee P_{n})}{P_{1} \vee \cdots \vee P_{n} \mid s}
\]

We will need two theorems to implement it:

\begin{itemize}
  \item \mintinline{lean}{dupOr {A B : Prop} : A ∨ A ∨ B → A ∨ B}
  \item \mintinline{lean}{dupOr' {A : Prop} : A ∨ A → A}
\end{itemize}

Also, in order to implement factor, we employ the tactic \texttt{pullToMiddle},
a generalization of \texttt{pull}. It allows its user to move a term in position $j$ in some clause to any position
$i < j$. The implementation of \texttt{pullToMiddle} is similar to the one for \texttt{pull}, except
that instead of grouping and ungrouping prefixes of the clause, it considers intervals in the middle of the clause.

The idea we employed to develop this tactic is the following: for each literal in the clause, we
check every other literal to see if it is equal to the current one. If it is, we obtain a new
clause with the literals that are equal adjacent to each other. We then apply either \texttt{dupOr}
or \texttt{dupOr'} (depending whether they are on the last positions of the clause), together
with the appropriate congruence lemmas (like we did for grouping clauses). We repeat this process
until there are no more duplicates in the clause.

The pseudocode in Figure~\ref{factorCore} shows our implementation of this idea.
First, in line 1, we obtain the clause corresponding to \texttt{pf} using \texttt{inferType},
in the same way we did in the implementation of \texttt{groupClausePrefix}. Then, in lines 2
to 15 iterate through each term in the clause.
We use the function $GetLength$ to obtain the current length of the clause. We need to
provide the index of the last proposition to this function, otherwise the length of the clause
will not be well defined.
The goal of each iteration is to remove all other elements of the clause that are equal to
the $i$-th one (the current one). For that, we do another nested loop in lines 5 to 14,
iterating through all indices $j$ such that $j > i$. Next, in line 6, we check whether
the propositions at positions $i$ and $j$ are equal (we use the notation $clause_{i}$ to
indicate the proposition at position $i$). This comparison is syntactic, that is, we
only consider equal propositions that have \textit{exactly} the same representation
as \texttt{Expr}. This is the intended behaviour of this rule. It is designed to
remove duplications introduced by applications of resolution (which will be syntactically equal).
If the propositions are different, we simply increment $j$. Otherwise,
we bring the $j$-th proposition to position $i + 1$ using the tactic \texttt{pullToMiddle}.
This will allow us to use one of the \texttt{dupOr} theorems. If $i + 1$ is the last
position in the clause, we apply \texttt{dupOr'}, otherwise we apply \texttt{dupOr}.
Notice that, since there are potentially other terms to the left of the $i$-th one,
we need to apply \texttt{congOrLeft} composed with \texttt{dupOr}, in the same way we
did for \texttt{groupClausePrefix}. The function $ApplyDupOr$ performs these checks
and applies the correct version of \texttt{dupOr}, producing a new proof term for the clause
with the duplicate erased. Since the number of elements to the left
of the last one necessarily decreased by one, we have to decrement $s$,
which is done in line 9. Finally, we update $clause$
according to our modifications by extracting again the type of $pf$.

Notice that the outer loop maintains following invariant: during the $i$-th iteration,
all propositions on the $i$-th prefix are distinct. Therefore, it is not necessary to
check propositions with index lesser than $i$.

\begin{figure}[t]
\begin{algorithmic}[1]
\Function{FactorCore}{$pf$, $s$}
  \State $clause \gets $ \Call{InferType}{$pf$}
  \For{$i \gets 1 $ \textbf{ up to } \Call{GetLength}{$clause$, $s$}}
    \State $j \gets i + 1$
    \While{$j < $ \Call{GetLength}{$clause$, $s$}}
      \If{$clause_{i} = clause_{j}$}
        \State $pf \gets $ \Call{PullToMiddleCore}{$pf$, $i + 1$, $j$, $s$}
        \State $pf \gets $ \Call{ApplyDupOr}{$pf$, $i$, $i + 1$}
        \State $s \gets s - 1$
        \State $clause \gets $ \Call{InferType}{$pf$}
      \Else
        \State $j \gets j + 1$
      \EndIf
    \EndWhile
  \EndFor
  \State \Return $pf$
\EndFunction
\end{algorithmic}\label{factorCore}
\end{figure}

The following snippet shows an application of this tactic:

\begin{minted}{lean}
  theorem factor1 :
      A ∨ B ∨ (E ∨ F) ∨ B ∨ A ∨ (E ∨ F) → A ∨ B ∨ (E ∨ F) :=
    by intro h
       factor h, 5
\end{minted}

Here, the tactic was used to eliminate the second occurrences of \texttt{A}, \texttt{B} and
\texttt{(E ∨ F)} at the hypothesis \texttt{h}.


\paragraph{Comparison with the certified approach.} An essential observation is that, while proving the theorems that corresponds to the three rules of Boolean reasoning appears to be a highly
challenging task, the implementations of the tactics are not too complicated (the implementation of \texttt{permutateClause},
in particular, consists of fewer than 60 lines). This goes to show the flexibility we obtain by not requiring
that all rules are proved before hand.

\section{Linear Arithmetic reasoning}

In this section we present three tactics regarding Linear Arithmetic reasoning.
The first two, \texttt{sumBounds} and \texttt{arithMulPos}/\texttt{arithMulNeg},
are used to derive false by combining the inequalities in the problem scaled by
the Farkas' coefficients, as we explained in Section~\ref{sec:liaCert}.
The third one, \texttt{tightBounds}, is a fact assumed during the process of
solving formulas involving integers.

\subsection{Summing lists of inequalities}

Given a list of $n$ proofs of statements following one of the patterns $a_{i} < b_{i}$, $a_{i} \le b_{i}$ or $a_{i} = b_{i}$,
the tactic \texttt{sumBounds}\footnote{The documentation for the corresponding cvc5 rule can be found at \url{https://cvc5.github.io/docs/cvc5-1.0.2/proofs/proof\_rules.html\#\_CPPv4N4cvc58internal6PfRule12ARITH\_SUM\_UBE}}
produces a proof of the inequality $\sum_{i = 1}^{n} a_{i} \bowtie^{*} \sum_{i = 1}^{n} b_{i}$, where $\bowtie^{*}$ is $<$ if
all terms in the premisses are strict inequalities and $\le$ otherwise. Its precise statement is the following:

\[
  \infer[]{\sum_{i = 1}^{n} a_{i} \bowtie^{*} \sum_{i = 1}^{n} b_{i}}{\bigwedge_{i = 1}^{n} a_{i} \bowtie_{i} b_{i}}
\]

\paragraph{Representing Int and Real.} Each one of the variables $a_{i}$ and $b_{i}$ might
have sort either \textit{Int} or \textit{Real}.
While each pair $a_{i}$ and $b_{i}$ always share the same sort, it is possible that some of these pairs have a sort different from the rest. Before implementing a
tactic corresponding to this rule, we have to choose types in Lean to match
these two sorts. We have based our decision on the types used by mathlib, as
individuals formalizing new mathematical statements in this library are one of the main potential users
of a Lean hammer. Integers are represented in mathlib exclusively with the built-in type \textit{Int}, so we will use it to represent the Int sort.
For the Real sort, we could either represent it using the type \texttt{Real}, which was defined in mathlib
to represent real numbers, or the type \texttt{Rat}, which was defined in the package \textit{std4}\footnote{std4 is Lean's standard library. It can be accessed at: \url{https://github.com/leanprover/std4}} to
represent rational numbers and is used by mathlib to formalize a variety of theorems
that involve this type of number.
It is not a problem to use rational numbers to represent the Real sort since the rules employed by cvc5 in the theory of linear arithmetic
do not explore any particular property that is enjoyed by real numbers and not by
rational numbers.
We decided to employ the type \texttt{Rat}, as its definition is much simpler and easier
to manage in comparison to the one for \texttt{Real}.

The implementation of this tactic will require 9 variations of the following theorem:

\begin{minted}{lean}
  sumBoundsThm {α : Type} [LinearOrderedRing α] {a b c d : α} :
    a < b → c < d → a + c < b + d
\end{minted}

Each variation will correspond to one combination of the relation symbols in the hypothesis, where
they can be either $<$, $\le$ or $=$. The relation symbol in the
conclusion is adapted accordingly in each theorem. Since the rule accepts mixing of variables
from Int and Real sort, we need a variation of each one of those 9 theorems for each combination
of the types of the variables. Instead of stating all the combinations explicitly, which would
result in a total of 36 theorems and a long branch in the implementation of the tactic,
we stated only one polymorphic version of each, as indicated by the type parameter \texttt{α} in \texttt{sumBoundsThm}.
Obviously, the theorem does not hold
for any \texttt{α} (it cannot even be stated if there is no comparison and addition operators defined over \texttt{α}).
We solve this issue by adding a restriction, stating that \texttt{α} satisfies the axioms of a
\textit{Linear Ordered Ring} (a class of types that contains both \texttt{Int} and \texttt{Rat} defined in mathlib), represented by the expression \texttt{[LinearOrderedRing α]} in the theorem. With this restriction we
could find a proof for each theorem.

\begin{figure}[t]
\begin{minted}[linenos]{lean}
  def combineBounds (pf₁ pf₂ : Expr) : MetaM Expr := do
    let t₁ ← inferType pf₁
    let t₂ ← inferType pf₂
    let rel₁ ← getRel t₁
    let rel₂ ← getRel t₂
    let opType₁ ← getOperandType t₁
    let opType₂ ← getOperandType t₂
    let (pf₁', pf₂') ← castHypothesis pf₁ pf₂ rel₁ rel₂ opType₁ opType₂
    let thmName : Name :=
      match rel₁, rel₂ with
      | `LT.lt , `LT.lt => `sumBoundsThm₁
      | `LT.lt , `LE.le => `sumBoundsThm₂
      | _      , _      => panic! "[sumBounds]: invalid relation"
    mkAppM thmName #[pf₂', pf₁']

  def sumBoundsCore (acc : Expr) (pfs : List Expr) : MetaM Expr :=
    match pfs with
    | [] => return acc
    | pf' :: pfs' => do
      let acc' ← combineBounds acc pf'
      sumBoundsCore acc' pfs'
\end{minted}
\caption{Implementation of the \texttt{sumBounds} tactic}\label{sumBoundsTac}
\end{figure}

The idea we employed to develop this tactic is the following: we set the last
proof in the input as an accumulator. Then, we iterate through the rest of the proofs
in reverse order. For each proof we go through, we produce a new term, adding the
inequality represented by this proof with the one in the accumulator. This is done
through one of the \texttt{sumBoundsThm} theorems. We must do it in reverse order
because addition is right-associative in Lean by default, and we can preserve
obtain a term in which all additions are associating to the right by iterating
in reverse order and always adding the new terms to the left of the inequality
represented by the accumulator.


Figure~\ref{sumBoundsTac} shows our implementation of this idea. The function
\texttt{sumBoundsCore} is supposed to be invoked with the last proof in the input
as \texttt{acc} and the rest of them, in reverse order, as \texttt{pfs}.
If \texttt{pfs} is empty, we just return our accumulator (line 18). Otherwise, we
invoke our function \texttt{combineBounds} to sum the inequalities represented
by \texttt{acc} and \texttt{pf'} (line 20) and recursively call \texttt{sumBoundsCore},
updating the arguments (line 21). The first action performed in function \texttt{combineBounds} is to obtain the inequalities corresponding to \texttt{pf₁} and
\texttt{pf₂} through \texttt{inferType}. Then, in lines 4 to 7, it inspects their structure and
obtain their relation symbol (\texttt{rel₁} and \texttt{rel₂}) and the type of their
operands (\texttt{opType₁} and \texttt{opType₂}). Notice that \texttt{sumBoundsThm}
expects that all the four variables have the same type. If one of the input proofs
represents an inequality over integers and the other is over rationals, we
have to lift the inequality between integers into an inequality between rationals.
This is done using one of the following theorems, depending on the relation symbol:

\begin{itemize}
  \item \mintinline{lean}{Int.castLT : ∀ {a b : Int}, a < b → Rat.ofInt a < Rat.ofInt b}
  \item \mintinline{lean}{Int.castLE : ∀ {a b : Int}, a ≤ b → Rat.ofInt a ≤ Rat.ofInt b}
  \item \mintinline{lean}{Int.castEQ : ∀ {a b : Int}, a = b → Rat.ofInt a = Rat.ofInt b}
\end{itemize}
where \texttt{Rat.ofInt} is the standard function to cast an integer into a rational.
The function \texttt{castHypothesis} (line 8) does this analysis and, if necessary,
applies one of these theorems to \texttt{pf₁} or \texttt{pf₂}.
Once we have the correct version of the proofs, we can apply one of the versions of
\texttt{sumBoundsThm}, depending on \texttt{rel₁} and \texttt{rel₂}. The \texttt{match}
statement in lines 10 to 13 chooses which version to apply. We only show two cases
for brevity. The backtick is used to transform a literal into a value of the built-in type
\texttt{Name}, that is employed by Lean to represent identifiers. Finally, in line 14, we apply the
chosen theorem to \texttt{pf₂'} and \texttt{pf₁'}. We have to invert the order here to compensate the fact that we are using these proofs in reverse order, compared to
the order they came in the input. The function \texttt{mkAppM} is a built-in function
from the metaprogramming framework, which tries to infer the implicit arguments for a
given function application. In this case, it succeeds, since they can be inferred
from the type of \texttt{pf₁'} and \texttt{pf₂'}.

\subsection{Scaling inequalities}

Given the variables \texttt{m}, \texttt{l} and \texttt{r} and an index \texttt{id},
we present two tactics for multiplying an inequality between \texttt{l} and \texttt{r}
by \texttt{m}. The parameter \texttt{id} is used to indicate what is the inequality
between \texttt{l} and \texttt{r} in question (lesser than, lesser or equal, greater than, or greater or equal). It is necessary since the rule\footnote{Its documentation can be found at \url{https://cvc5.github.io/docs/cvc5-1.0.2/proofs/proof\_rules.html\#\_CPPv4N4cvc58internal6PfRule14ARITH\_MULT\_POSE}} corresponding to this tactic does not receive the proof of the inequality
as a premise, instead, it produces an implication from such an inequality to its
scaled version. The first tactic, \texttt{arithMulPos}, assumes that \texttt{m} is positive. Its precise
statement is the following:

\[
\infer[]{(m > 0 \wedge l \bowtie_{id} r) \rightarrow m \cdot l \bowtie_{id} m \cdot r}{- \mid m, l, r, id}
\]

The second tactic, \texttt{arithMulNeg}, assumes that \texttt{m} is negative. Its precise statement is the following:

\[
\infer[]{(m < 0 \wedge l \bowtie_{id} r) \rightarrow m \cdot l \bowtie_{inv(id)} m \cdot r}{- \mid m, l, r, id}
\]
where $\bowtie_{inv(id)}$ is $>$ if $\bowtie_{id}$ is $<$, $\ge$ if $\bowtie_{id}$ is $\le$ and so on.

We could have just proved a theorem for this rule if every variable had the
same type, but this is not the case. Each one of them (\texttt{m}, \texttt{l} and \texttt{r}) can be either an \texttt{Int} or a \texttt{Rat}. This tactic inspects
the type of each one of the variables and apply one of the eight corresponding theorems,
accordingly, in a manner similar to \texttt{sumBounds}. All these theorems are
easy to be proved.


\subsection{Tightening bounds}

Given an integer \texttt{i}, a numeric variable \texttt{q} and a proof of a strict inequality
between \texttt{i} and \texttt{q}, the pair of tactics \texttt{tightLb} and \texttt{tightUb} produce proofs of a tighter relation
between \texttt{i} and \texttt{q}. More specifically, the following are the precise
statements of \texttt{tightLb} and \texttt{tightUb}, respectively:

\[
\infer[]{i \le \lceil q \rceil - 1}{i < q \mid i, q}
\]


\[
\infer[]{i \ge \lfloor q \rfloor + 1}{i > q \mid i, q}
\]

Once again, this tactic could be just a theorem, except that cvc5 can apply it
with variables from both Real and Int sorts as the parameter \texttt{q}. The logic
of this tactic consists of checking what is the type of \texttt{q} and applying the
appropriate theorem.

Unlike the previous theorems, this one was not easy to prove. In fact, when we were
developing this tactic, the implementation of the \textit{ceil} and \textit{floor}
functions we were using (from the package \textit{std}) contained errors\footnote{The pull request that fixed the errors is the following: \url{https://github.com/leanprover/std4/pull/80}}. We defined and used our own
versions of these functions to prove the theorems, and, when the version of
the library was fixed, we substituted our version by it.
Our original proofs had almost 350 lines combined, which was a
consequence of the fact that there were almost no theorems in mathlib about these
functions. Once more theorems were added to the library, we could reduce each proof
to 15 lines.

\section{Equality reasoning}




% proof of $P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n} \rightarrow (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$, together with \texttt{congOrLeft} to surpass
% the propositions to the left of $P_{i - 1}$. We need one application of \texttt{congOrLeft}
% for each proposition to the left of $P_{i - 1}$. For instance, if:
% \begin{itemize}
%   \item \textbf{i = 2} our term will be \texttt{orAssoc}
%   \item \textbf{i = 3} our term will be \texttt{congOrLeft orAssoc}
%   \item \textbf{i = 4} our term will be \texttt{congOrLeft (congOrLeft orAssoc)}
% \end{itemize}


% The idea is to use \texttt{congOrLeft} to surpass the propositions to the left of $P_{i - 1}$ together with \texttt{orAssoc} instantiating \texttt{A} to $P_{i - 1}$, \texttt{B} to $P_{i}$ and \texttt{C} to $P_{i + 1} \vee \cdots \vee
% P_{n}$, which will yield a proof of $P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n} \rightarrow (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.


% We cannot directly apply it to $h$ since there are potentially other terms to the left of $P_{i - 1}$.
% To solve this problem, we use the theorem \texttt{congOrLeft} setting \texttt{hyp} to the application of \texttt{orAssoc} that we just described.
% Since the first $i - 2$ propositions are not grouped, we cannot instantiate \texttt{C} to $P_{i - 1} \vee \cdots \vee P_{i - 2}$.
% Instead, we set \texttt{C} to $P_{i - 2}$, which will yield a proof \texttt{h₂} of $P_{i - 2} \vee P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n} \rightarrow P_{i - 2} \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$. Now we repeat the process applying \texttt{congOrLeft} with \texttt{C} instantiated to $P_{i - 3}$ (if it exists) and \texttt{hyp} to \texttt{h₂}. We keep going until we set \texttt{C} to $P_{1}$. We will then obtain a term of the form \texttt{congOrLeft (congOrLeft \ldots (congOrLeft orAssoc)\ldots)} (with $i - 2$ applications of \texttt{congOrLeft}), which, when applied to \texttt{h}, will prove $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.


% One detail that has to be considered while implementing this tactic is that the index of the last proposition
% may change when we pull a term. For instance, the index of the last proposition of the clause
% $A \vee B \vee (C \vee D) \vee E$ is $4$. If we pull $E$ in this clause, we will get
% $E \vee A \vee B \vee (C \vee D)$, which has $3$ as this index. If this happens,
% the new index will always be the original length of the clause minus the length
% of the second to last proposition in the current clause (since it will become the last proposition).
% While executing the procedure we described in the last paragraph, we need to always keep track of the
% current index of the last proposition, as the tactic pull requires it as an argument.

% \begin{figure}[t]
% % \textbf{Input:} $\psi$, a PL formula\\
% % \textbf{Output:} \textit{true} or \textit{false}, depending whether $\psi$ is satisfiable
% \begin{algorithmic}[1]
% \Function{PermutateClauseCore}{$pf$, $perm$, $s$}
%   \State $clause \gets $ \Call{InferType}{$pf$}
%   \State $lenClause \gets $ \Call{getLength}{$clause$}
%   \For{$i \gets n \Downto 1$}
%   \If{$perm_{i} = s$}
%     \State $currentClause \gets$ \Call{InferType}{pf}
%     \State $lenSecondLast \gets $ \Call{GetLength}{$currentClause_{s - 1}$}
%     \State $pf \gets$ \Call{PullCore}{$pf$, $perm_{i}$, $s$}
%     \State $s \gets lenSecondLast - lenClause$
%   \Else
%     \State $pf \gets$ \Call{PullCore}{$pf$, $perm_{i}$, $s$}
%   \EndIf
%   \EndFor
%   \State \Return~$pf$
% \EndFunction
% \end{algorithmic}
% \caption{Implementation of the tactic PermutateClause}~\label{permClause}
% \end{figure}
