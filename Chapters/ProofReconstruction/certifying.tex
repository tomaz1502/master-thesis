In this chapter we describe our main contribution, which is a module
for lifting the scripts presented in Section~\ref{sec:gen-scripts} into
Lean proofs, based on the certifying transformations approach.
%
The specific manner in which we applied
this concept in our context was to develop a set of tactics matching
the rules present in cvc5's proof calculus.
A cvc5 proof is then a sequence of applications of those tactics.
When we ask Lean's kernel to check the proof script, it will execute the tactics
one by one. Each one of them inspects the current context
and produces a proof term corresponding to a proof of a specific
case of the rule represented by that tactic.
The checker then verifies the correctness of each proof, closing the original goal only
if all checks were succesful.

There are two options for implementing these tactics: the first one is to generate
a value of type \texttt{Syntax} (that is, a piece of Lean's code) that proves
the theorem in question and invoke \texttt{evalTactic} using this value;
the second option is to craft an \texttt{Expr} that corresponds to a value
of the appropriate type. In general,
it is simpler to generate \texttt{Syntax}, as the elaborator will fill in
some details for us, such as the implicit arguments for functions. On the
other hand, we can skip calls to the elaborator if we produce an \texttt{Expr}
instead, which will potentially lead to a gain in performance. With this in mind,
we decided to employ the second option.


Another important difference from the certified transformations approach is that we do not use the \texttt{term}
type anymore. The main reason for mapping MSFOL formulas to this inductive type
instead of native Lean expressions is the flexibility achieved by this representation.
While it is straightforward to define a function that inspects and manipulates the structure of
a \texttt{term} by pattern matching, there is no way to do the same for certain Lean types (\texttt{Prop}, for
instance) without recurring to metaprogramming. As we have shown in Section~\ref{sec:metaLean},
the metaprogramming context grants us access to the internal representation of any expression
through the \texttt{Expr} type, which can be inspected and manipulated in the same way
as \texttt{term}. Since the framework for writing tactics is based on metaprogramming, we
do not need to rely on the flexibility of the \texttt{term} type. Therefore, we
have decided to not use the deep embedding anymore, and translate MSFOL formulas directly
to Lean expressions.

The snippet in Figure~\ref{notModusPonens2} shows an example of proof in the new format.
It corresponds to the proof that cvc5 produces to the SMT-Lib query presented
in Figure~\ref{negModusPonens}. A considerable portion of the rules (including
every rule regarding CNF transformation) can be easily proved using classical
reasoning. Instead of mapping such rules to tactics, we just proved them as theorems.
Those theorems will not be presented here. For a complete overview of the rules, their
statement and whether they were implemented with a tactic or a
theorem, refer to Table~\ref{tab:rules}.
All the rules present in the proof in Figure~\ref{notModusPonens2} were mapped to
theorems except for resolution (line 8), which was mapped to a tactic that we will
present.


\begin{figure}[t]
\begin{minted}[linenos]{lean}
  theorem cvc5_th0 {P Q : Prop} : (Not (P → ((P → Q) → Q))) → False :=
    fun lean_a0 : (Not (P → ((P → Q) → Q))) => by
      have lean_s0 : (Not ((P → Q) → Q)) := notImplies2 lean_a0
      have lean_s1 : (P → Q)             := notImplies1 lean_s0
      have lean_s2 : (Or (Not P) Q)      := impliesElim lean_s1
      have lean_s3 : P                   := notImplies1 lean_a0
      have lean_s4 : Q                   :=
        by R2 lean_s2, lean_s3, P, [1, 0]
      have lean_s5 : (Not ((P → Q) → Q)) := notImplies2 lean_a0
      have lean_s6 : (Not Q)             := notImplies2 lean_s5
      exact (show False from contradiction lean_s4 lean_s6)
\end{minted}
\caption{Proof script using certifying transformations.}\label{notModusPonens2}
\end{figure}

\paragraph{Representation of clauses.} Internally, clauses are represented by cvc5 as
a lists of terms. A list of terms can correspond to many distinct
clauses, depending on how you parenthesize them. Implicitly, cvc5 is using the
fact that disjunction is associative, which implies that all these clauses
are equivalent, therefore they do not need to be differentiated inside the calculus.
When these lists of terms are sent to Lean, we are forced to chose a way to parenthesize
those terms and build our proofs taking into account this format.
We chose to parenthesize them in a right-associative way.
Therefore, if cvc5 is
representing some clause as the list $[t_{1}, t_{2}, t_{3}, t_{4}]$ we will represent it in Lean as
\texttt{t₁' ∨ (t₂' ∨ (t₃' ∨ t₄'))}, where \texttt{tᵢ'} is the representation of $t_{i}$.
Since the built-in operator \texttt{∨} is right-associative, we can omit the
parenthesis. Throughout the implementation of the tactics, we always assume
that a clause that is received as premise comes parenthesized in this way,
and every clause that is proved by a tactic also has this format.

In the rest of this chapter we will give an overview of the implementation of some of the
tactics. We present their statements with the same format used in cvc5's
documentation\footnote{The documentation of cvc5's rules can be found at:
  \url{https://cvc5.github.io/docs/cvc5-1.0.2/proofs/proof_rules.html}}, that is,
for a tactic that has a conclusion $\psi$, premisses $\psi_{1}, \cdots, \psi_{n}$ and
parameters $t_{1}, \cdots, t_{n}$, we write:
\[
  \infer[]{\psi}{\psi_{1}, \cdots, \psi_{n} \mid t_{1}, \cdots, t_{n}}
\]

\section{Auxiliary Tactics}

This first class of tactics does not correspond to cvc5's rules. They were implemented
to facilitate the implementation of the main tactics.



\subsection{Parenthesizing prefixes of clauses}

Given a number \texttt{i} and a proof \texttt{pf}
of the validity of a clause, the application \texttt{groupClausePrefix pf, i} proves the same clause, with the
first $i$ propositions parenthesized. The disjunction of the first $i$ propositions is a single element of the output clause.  The precise statement of this rule
is the following:

\[
  \infer[]{(P_{1} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}}
    {P_{1} \vee \cdots \vee P_{n} \mid i}
\]

This tactic employs the following two theorems, which were easily proven:

\begin{itemize}
  \item \mintinline{lean}{orAssoc {A B C : Prop} : A ∨ B ∨ C → (A ∨ B) ∨ C}
  \item \mintinline{lean}{congOrLeft {A B C : Prop} (hyp : A → B) : C ∨ A → C ∨ B}
\end{itemize}

% Also, it will make use of the \texttt{apply} tactic. If the current goal is \texttt{Q}, and
% we have a proof \texttt{h} of \texttt{P → Q}, we can reduce the goal to \texttt{P} with
% \texttt{apply h}.

% Let's instantiate, in the theorem \texttt{orAssoc}, \texttt{A} to $P_{1}$, \texttt{B}
% to $P_{2} \vee \cdots \vee P_{i}$ and \texttt{C} to $P_{i + 1} \vee \cdots \vee P_{n}$. This will yield a proof of
% of $P_{1} \vee (P_{2} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n} \rightarrow (P_{1} \vee \dots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$. Since
% the proof script is annotated with the type it expects from each tactic application,
% we can assume that the current goal is $(P_{1} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$. Therefore, we can change the goal to $P_{1} \vee (P_{2} \vee \cdots \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$ with \texttt{apply orAssoc} (Lean's kernel can infer what are the implicit arguments in this case). Now we would like to apply \texttt{orAssoc} again to remove $P_{2}$ from the parenthesized group. We cannot do it directly, due to the term $P_{1}$ to the left of the parenthesis. We solved this
% problem by using \texttt{congOrLeft}, with \texttt{C} instantiated to $P_{1}$ and
% \texttt{hyp} instantiated to \texttt{orAssoc}, which yields a proof of the correct
% statement. Therefore, we can change the goal to $P_{1} \vee P_{2} \vee (P_{3} \vee \cdots \vee P_{i}) \vee \cdots \vee P_{n}$ with \texttt{apply (congOrLeft orAssoc)}.

First, let's build a proof term for $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$. We instantiate, in \texttt{orAssoc}, the parameter \texttt{A} to $P_{i - 1}$, \texttt{B} to $P_{i}$ and \texttt{C} to $P_{i + 1} \vee \cdots \vee P_{n}$, obtaining the term:

\begin{center}
    $f_{1}: P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n}   \rightarrow (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n} $
\end{center}

Then, we apply \texttt{congOrLeft} to $f_{1}$ instantiating \texttt{C} to $P_{i - 2}$, which produces:

\begin{center}
    $f_{2}: P_{i - 2} \vee P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n}   \rightarrow P_{i - 2} \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n} $
\end{center}

We repeat this process until we get the term $f_{i - 1}$ of type $P_{1} \vee \cdots \vee P_{n} \rightarrow P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$. Applying $f_{i - 1}$ to the original hypothesis \texttt{h} yields a term \texttt{h'} with type $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.

A clause is composed by any kind of propositions, including disjunctions.
Therefore, the term $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$ is a new clause
with $n - 1$ propositions, in which the $(i - 1)$-th proposition is the disjunction
$P_{i - 1} \vee P_{i}$. This means that our original problem can be reduced to
group the first $i - 1$ terms of this clause.
With this in mind, we repeat the process of composing \texttt{congOrLeft} with
\texttt{orAssoc} and apply the result to \texttt{pf'}, obtaining
a new term of type $P_{1} \vee \cdots \vee (P_{i - 2} \vee P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.
We keep repeating this process until the whole prefix is grouped.


\begin{figure}[t]
\begin{minted}[linenos]{lean}
def groupPrefixCore (pf : Expr) (i : Nat) : MetaM Expr := do
  let clause  : Expr      ← inferType pf
  let props   : List Expr ← collectPropsInClause clause
  if i > 0 && i < List.length props then
    let lemmas : List Expr ← groupPrefixLemmas props (i - 1)
    let answer : Expr :=
      List.foldl (fun acc lem => Expr.app lem acc) pf lemmas
    return answer
  else throwError
    "[groupClausePrefix]: invalid prefix length"
\end{minted}
\caption{Implementation of the tactic GroupClausePrefix}\label{groupClause}
\end{figure}

The snippet in Figure~\ref{groupClause} and shows the
implementation of the core functionality of this tactic.
The parameters \texttt{pf} and \texttt{i} of the function \texttt{groupPrefixCore}
represent, respectively, the proof of the validity of the original clause
and the length of the prefix that should be grouped. In line 2, we obtain
an \texttt{Expr} corresponding to the original clause by inspecting the type
of \texttt{pf}, with the built-in routine \texttt{inferType}. Then, in line 3,
we use the function \texttt{collectPropsInClause} (defined by us) to
extract a \texttt{List} with each one of the propositions in the clause.
Next, we check if the prefix length is valid. If it is, we apply our function
\texttt{groupPrefixLemmas} to obtain a list of expressions. Each element in this list
is a composition of \texttt{congOrLeft} and \texttt{orAssoc} described before.
One important difficulty in the implementation of this function is that implicit parameters cannot always be automatically computed in the \texttt{Expr} level, so
this function also computes some of the implicit arguments that have to be
passed to \texttt{congOrLeft} and \texttt{orAssoc}. Lean has a built-in
functionality to automatically infer part of those arguments, but a significant
amount of them have to be manually constructed.
Finally, we use \texttt{foldl}
in line 7 to apply each one of the lemmas to \texttt{pf}, accumulating the results.

In order to use \texttt{groupPrefixCore} as a tactic, we implement
a function of type \texttt{Syntax → TacticM Unit}, which parses the \texttt{Syntax}
to obtain \texttt{pf} and \texttt{i} and use the \texttt{Expr} generated by
\texttt{groupPrefixCore} to close the current goal.

The following snippet shows an example of usage of this tactic:

\newpage

\begin{minted}{lean}
  theorem group3 : A ∨ B ∨ C ∨ D ∨ E → (A ∨ B ∨ C) ∨ D ∨ E := by
    intro h
    groupClausePrefix h, 3
\end{minted}

We can use the \texttt{\#print} command to inspect the proof term generated by the tactic, and
verify that it is, indeed, the one we described:

\begin{minted}{lean}
  #print group3 -- fun {A B C D E} h => orAssoc (congOrLeft orAssoc h)
\end{minted}
% \tom{proof term size is $\mathcal{O}(i^{2})$. Maybe say a few words about this?}

\subsection*{Moving terms inside clauses}

Given a proof \texttt{pf} of the validity of a clause and an index \texttt{i} of a proposition to be moved, the \texttt{pull} tactic should produce a proof term for the same clause,
with $i$-th term moved to the first position. This tactic also facilitates the implementation
of the other tactics and does not exist in cvc5's proof calculus. The precise
statement of this rule is the following:

\[
  \infer[]{P_{i} \vee P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n}}{P_{1} \vee \cdots \vee P_{n}  \mid i, s}
\]


We need three theorems to implement \texttt{pull}, which are easy to prove:

\begin{itemize}
  \item \mintinline{lean}{orComm {A B C : Prop} : A ∨ B → B ∨ A}
  \item \mintinline{lean}{congOrRight {A B C : Prop} (hyp : A → B) : A ∨ C → B ∨ C}
  \item \mintinline{lean}{orAssocConv {A B C : Prop} : (A ∨ B) ∨ C → A ∨ B ∨ C}
\end{itemize}

To build the required proof term, let's first apply our tactic \texttt{groupClausePrefix} at \texttt{pf}, grouping the prefix of length $i - 1$.
We cannot use the tactic itself, as we are working on the context of \texttt{MetaM}, but we can invoke directly the function \texttt{groupPrefixCore}
to produce the required term. We will obtain the following term:

\begin{center}
  $pf_{1} : (P_{1} \vee \cdots \vee P_{i - 1}) \vee P_{i} \vee \cdots \vee P_{n}$
\end{center}

Then, we use again our function \texttt{groupPrefixCore} in \texttt{pf₁}, grouping the prefix of length 2. This provides the term:

\begin{center}
  $pf_{2} : ((P_{1} \vee \cdots \vee P_{i - 1}) \vee P_{i}) \vee P_{i + 1} \vee \cdots \vee P_{n}$
\end{center}

Next, we will define a new term \texttt{pf₃} as \texttt{congOrRight orComm pf₂}. Given the appropriate instantiation for the implicit parameters,
this application flips the position of $P_{1} \vee \cdots \vee P_{i - 1}$ and $P_{i}$, while not modifying the suffix $P_{i + 1} \vee \cdots \vee P_{n}$.
Therefore, \texttt{pf₃} has type $(P_{i} \vee P_{1} \vee \cdots \vee P_{i - 1}) \vee P_{i + 1} \vee \cdots \vee P_{n}$ (we do not need to parenthesize
$P_{1} \vee \cdots \vee P_{i - 1}$ as $\vee$ is right-associative). Finally we use another tactic we implemented, \texttt{ungroupClausePrefix},
to remove the parenthesis around $P_{i} \vee P_{1} \vee \cdots \vee P_{i - 1}$, concluding our goal.
This tactic is similar to \texttt{groupClausePrefix}. The
difference is that it substitutes the theorem \texttt{orAssoc} by \texttt{orAssocConv} and it applies the theorems in reverse order, i.e.\ it
starts with an application of \texttt{orAssocConv}, then it applies \texttt{congOrLeft orAssocConv} and so on.

\paragraph{Disambiguating clauses.} The difference between the representation of
clauses in cvc5 and in Lean leads to an ambiguity in applications of this tactic.
While there is no distinction in Lean between, for instance, the clauses $A \vee B \vee C \vee D \vee E$
and $A \vee B \vee C \vee (D \vee E)$, this distinction can be made inside the SMT solver. Indeed, if
we have a term \texttt{pf} in Lean of type $A \vee B \vee C \vee D \vee E$ and cvc5 prints the tactic
application \texttt{pull pf, 4}, there is no way to know, inside Lean, if it is expecting to receive
a proof of $D \vee A \vee B \vee C \vee E$ or $(D \vee E) \vee A \vee B \vee C$. To address this issue,
we adapted cvc5 to print an extra parameter \texttt{s} to this tactic (and to others that suffered from the same problem),
which is a natural number corresponding to the index of the last proposition in the clause from the
point of view of the SMT solver. Therefore, for the example we just described, it would print
\texttt{pull pf, 4, 4} to indicate that it wants a proof for $(D \vee E) \vee A \vee B \vee C$ and
\texttt{pull pf, 4, 5} to indicate that it wants a proof for $E \vee A \vee B \vee C \vee D$.

\section{Boolean reasoning}

We now present the tactics regarding Boolean reasoning. One of them corresponds to
the resolution rule, which as we have shown in Section~\ref{sec:pcBool}, is the main
building block for proofs in Propositional Logic. The other two, \textit{factor} and
\textit{permutateClause}, are used to justify implicit steps that SAT solvers
perform while solving formulas through resolution.

\subsection*{Reordering clauses}

Given the proof \texttt{pf} of a clause, a permutation \texttt{perm} and the index \texttt{s} of the last
proposition in the clause, the application \texttt{permutateClause pf, perm, s} provides a proof for the same clause, with
the propositions permutated according to \texttt{perm}.
This is the first tactic that matches a rule used by cvc5\footnote{The documentation of this rule can be found at \url{https://cvc5.github.io/docs/cvc5-1.0.2/proofs/proof\_rules.html\#\_CPPv4N4cvc58internal6PfRule10REORDERINGE}}. The precise statement
of this rule is the following:

\[
  \infer[]{P_{perm(1)} \vee \cdots \vee P_{perm(n)}}{P_{1} \vee \cdots \vee P_{n} \mid perm, s}
\]


Since we have \texttt{pull}, we can write a tactic matching
this rule using a relatively straightforward approach. We simply iterate through the permutation in
reverse order, and, for each index \texttt{i} we go through, we run the \texttt{pull} tactic
with \texttt{i} as the argument. With this strategy, the last term we will bring to the first position is
$P_{perm(1)}$, therefore, the final clause we will produce will have the correct element in the first position.
Similarly, the second to last element we will bring to the first
position is $P_{perm(2)}$. After pulling it, we will bring another term to the first position ($P_{perm(1)}$), which will make $P_{perm(2)}$
end up in the second position, as required. By extending this reasoning to all the terms in the clause,
we can conclude that each one of them will be placed in the right position. Notice that the index \texttt{s} of
the last proposition does \textit{not} change during this process, so we can always use \texttt{s} as the argument
required by \texttt{pull}.

\begin{figure}[t]
\begin{minted}[linenos]{lean}
  def pullProps (props : List Expr) (acc : Expr) (s : Nat) :
      MetaM Expr :=
    match props with
      | [] => return acc
      | e::es => do
          let pulled ← pullCore e acc s
          pullProps es pulled s

  def permutateClauseCore (pf : Expr) (perm : List Nat)
      (s : Nat) : MetaM Expr := do
    let clause : Expr     ← inferType pf
    let props : List Expr ← collectPropsInClause' clause s
    let permutatedProps   := permutateList props (List.reverse perm)
    pullProps permutatedProps pf s
\end{minted}
\caption{Implementation of the tactic permutateClause.}\label{permClauseImp}
\end{figure}

The code in Figure~\ref{permClauseImp} shows our implementation of this tactic.
The main function is \texttt{permutateClauseCore}. It starts by obtaining a list of
\texttt{Expr} corresponding to the terms in the clause using \texttt{inferType}
and \texttt{collectPropsInClause} (lines 11 and 12), in the same manner as
\texttt{groupPrefixCore}. Next, in line 13, it permutates this list according
to the reverse of the input permutation. Finally, in line 14, it invokes
\texttt{pullProps} with the permutated list, \texttt{pf} and \texttt{s}.
This auxiliary method traverses the input list, applying \texttt{pull} to
each \texttt{Expr} it encounters, accumulating the result.

The following snippet shows an example of usage of this tactic:

\begin{minted}{lean}
  theorem perm1 :
      A ∨ B ∨ C ∨ (D ∨ E ∨ F) → (D ∨ E ∨ F) ∨ B ∨ C ∨ A := by
    intro h
    permutateClause h, [3, 1, 2, 0], 3
\end{minted}

\paragraph{Comparison with the certified approach.} An essential observation is that, while proving the theorem that corresponds to this rule appears to be a
challenging task, the complete implementation of the tactic was notably compact and uncomplicated, consisting of fewer
than 60 lines (excluding the source code of \texttt{pull}).


\subsection*{Resolution}

Given two proofs of clauses, \texttt{pfP} and \texttt{pfQ}, a proposition \texttt{A}
(also known as the \textit{pivot}) such that
\texttt{A} is an element of the first clause and \texttt{¬ A} is an element of the second clause and the indices \texttt{s₁} and \texttt{s₂} of the last propositions in each clause, this tactic produces a proof for the clause composed by the concatenation
of the two, removing the pivot from the first and the negation of the pivot from the second. If there are multiple instances of the pivot in the first clause, it removes
the first one. Similarly, if there are multiple instances of the negation of the pivot
in the second clause, it also removes the first one. The precise statement
is the following:

\[
  \infer[]{P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n} \vee Q_{1} \vee \cdots \vee Q_{j - 1} \vee Q_{j + 1} \vee \cdots \vee Q_{m}}{P_{1} \vee \cdots \vee P_{i - 1} \vee A \vee P_{i + 1} \vee \cdots \vee P_{n}, Q_{1} \vee \cdots \vee Q_{j - 1} \vee \neg A \vee Q_{j + 1} \vee \cdots \vee Q_{m}, \mid A, s_{1}, s_{2}}
\]

We will need four theorems to implement the resolution tactic, which are all easy to prove:

\begin{itemize}
  \item \mintinline{lean}{resolutionSpecialCase1 {A B C : Prop} : A ∨ B → ¬ A ∨ C → B ∨ C}
  \item \mintinline{lean}{resolutionSpecialCase2 {A B : Prop} : A ∨ B → ¬ A → B}
  \item \mintinline{lean}{resolutionSpecialCase3 {A C : Prop} : A → ¬ A ∨ C → C}
  \item \mintinline{lean}{resolutionSpecialCase4 {A : Prop} : A → ¬ A → False}
\end{itemize}

First, we apply \texttt{pull} to \texttt{pfP} to bring the first occurence of \texttt{A} to the first position. This will result
in a proof \texttt{pfP'} of the clause:

\begin{center}
  $A \vee P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n}$
\end{center}

Next, we do the same at \texttt{pfQ}, bringing the negation of the pivot to the first position. We will obtain a proof \texttt{pfQ'} of
the clause:

\begin{center}
  $\neg A \vee Q_{1} \vee \cdots \vee Q_{j - 1} \vee Q_{j + 1} \vee \cdots \vee Q_{m}$
\end{center}

Now, we apply one of the \texttt{resolutionSpecialCase} theorems to \texttt{pfP'} and \texttt{pfQ'}. We decide which theorem
to apply based on whether or not each clause has other terms apart from the pivot. If both clauses have other terms (i.e. $n > 0$ and
$m > 0$) we apply \texttt{resolutionSpecialCase1}, instantiating \texttt{B} to $P_{1} \vee \cdots \vee P_{i - 1} \vee P_{i + 1} \vee \cdots \vee P_{n}$ and \texttt{C} to $Q_{1} \vee \cdots \vee Q_{j - 1} \vee Q_{j + 1} \vee \cdots \vee Q_{m}$. If the second clause consists of only the negation of the pivot and the first one has other terms, we apply \texttt{resolutionSpecialCase2}. If the first clause consists of only the pivot and the second one has other terms, we apply \texttt{resolutionSpecialCase3}. If both clauses consist of a single element, we apply \texttt{resolutionSpecialCase4}. In this case, the tactic will produce a proof of \texttt{False}, which is the way we represent the empty clause.

Notice that, if we apply \texttt{resolutionSpecialCase1} and the first clause had more than 1 element
apart from the pivot, the term we get is not the required one. Its type will have the
propositions from the first clause parenthesized, i.e.\ it will have the following form:

\begin{center}
  $(P_{1} \vee \cdots \vee P_{n}) \vee Q_{1} \vee \cdots \vee Q_{m}$
\end{center}

We fix this issue with the tactic \texttt{ungroupClausePrefix}, which was mentioned before.

The original rule used by cvc5 has another
boolean parameter, \textit{pol}. If pol is set to true, then the semantics of the rule matches
exactly the tactic we described. If pol is set to false, the rule expects to find the pivot
negated in the first clause and not negated in the second. We have implemented a separate
tactic for this case. The function that implements the core functionality of both tactics is
the same.

The following snippet shows one example of usage of each tactic. \texttt{R1} corresponds to
the rule with pol set to true, and \texttt{R2} corresponds to the rule with pol set to false.
The last parameter of the tactic is a list with two elements, corresponding to the indices
of the last propositions in each clause.

\begin{minted}{lean}
  theorem res1 : A ∨ B ∨ C ∨ D → E ∨ ¬ B → A ∨ (C ∨ D) ∨ E := by
    intros h₁ h₂
    R1 h₁, h₂, B, [2, 1]

  theorem res2 : ¬ A → B ∨ A ∨ C → B ∨ C := by
    intros h₁ h₂
    R2 h₁, h₂, A, [0, 2]
\end{minted}

\subsection*{Removing duplicates}

Given a proof \texttt{pf} of a clause and the index \texttt{s} of the last proposition in the clause,
the \textit{factor} tactic produces a proof of the same clause, with all duplicated literals of it removed. The precise statement of this rule is the following:

\[
  \infer[]{removeDuplicates(P_{1} \vee \cdots \vee P_{n})}{P_{1} \vee \cdots \vee P_{n} \mid s}
\]

We will need two theorems to implement it:

\begin{itemize}
  \item \mintinline{lean}{dupOr {A B : Prop} : A ∨ A ∨ B → A ∨ B}
  \item \mintinline{lean}{dupOr' {A : Prop} : A ∨ A → A}
\end{itemize}

Also, in order to implement factor, we employ the tactic \texttt{pullToMiddle},
a generalization of \texttt{pull}. It allows its user to move a term in position $j$ in some clause to any position
$i < j$. The implementation of \texttt{pullToMiddle} is similar to the one for \texttt{pull}, except
that instead of grouping and ungrouping prefixes of the clause, it considers intervals in the middle of the clause.

The idea we employed to develop this tactic is the following: for each literal in the clause, we
check every other literal to see if it is equal to the current one. If it is, we obtain a new
clause with the literals that are equal adjacent to each other. We then apply either \texttt{dupOr}
or \texttt{dupOr'} (depending whether they are on the last positions of the clause), together
with the appropriate congruence lemmas (like we did for grouping clauses). We repeat this process
until there are no more duplicates in the clause.

The pseudocode in Figure~\ref{factorCore} shows our implementation of this idea.
First, in line 1, we obtain the clause corresponding to \texttt{pf} using \texttt{inferType},
in the same way we did in the implementation of \texttt{groupClausePrefix}. Then, in lines 2
to 15 iterate through each term in the clause.
We use the function $GetLength$ to obtain the current length of the clause. We need to
provide the index of the last proposition to this function, otherwise the length of the clause
will not be well defined.
The goal of each iteration is to remove all other elements of the clause that are equal to
the $i$-th one (the current one). For that, we do another nested loop in lines 5 to 14,
iterating through all indices $j$ such that $j > i$. Next, in line 6, we check whether
the propositions at positions $i$ and $j$ are equal (we use the notation $clause_{i}$ to
indicate the proposition at position $i$). This comparison is syntactic, that is, we
only consider equal propositions that have \textit{exactly} the same representation
as \texttt{Expr}. This is the intended behaviour of this rule. It is designed to
remove duplications introduced by applications of resolution (which will be syntactically equal).
If the propositions are different, we simply increment $j$. Otherwise,
we bring the $j$-th proposition to position $i + 1$ using the tactic \texttt{pullToMiddle}.
This will allow us to use one of the \texttt{dupOr} theorems. If $i + 1$ is the last
position in the clause, we apply \texttt{dupOr'}, otherwise we apply \texttt{dupOr}.
Notice that, since there are potentially other terms to the left of the $i$-th one,
we need to apply \texttt{congOrLeft} composed with \texttt{dupOr}, in the same way we
did for \texttt{groupClausePrefix}. The function $ApplyDupOr$ performs these checks
and applies the correct version of \texttt{dupOr}, producing a new proof term for the clause
with the duplicate erased. Since the number of elements to the left
of the last one necessarily decreased by one, we have to decrement $s$,
which is done in line 9. Finally, we update $clause$
according to our modifications by extracting again the type of $pf$.

Notice that the outer loop maintains following invariant: during the $i$-th iteration,
all propositions on the $i$-th prefix are distinct. Therefore, it is not necessary to
check propositions with index lesser than $i$.

\begin{figure}[t]
\begin{algorithmic}[1]
\Function{FactorCore}{$pf$, $s$}
  \State $clause \gets $ \Call{InferType}{$pf$}
  \For{$i \gets 1 $ \textbf{ up to } \Call{GetLength}{$clause$, $s$}}
    \State $j \gets i + 1$
    \While{$j < $ \Call{GetLength}{$clause$, $s$}}
      \If{$clause_{i} = clause_{j}$}
        \State $pf \gets $ \Call{PullToMiddleCore}{$pf$, $i + 1$, $j$, $s$}
        \State $pf \gets $ \Call{ApplyDupOr}{$pf$, $i$, $i + 1$}
        \State $s \gets s - 1$
        \State $clause \gets $ \Call{InferType}{$pf$}
      \Else
        \State $j \gets j + 1$
      \EndIf
    \EndWhile
  \EndFor
  \State \Return $pf$
\EndFunction
\end{algorithmic}\label{factorCore}
\end{figure}

The following snippet shows an application of this tactic:

\begin{minted}{lean}
  theorem factor1 :
      A ∨ B ∨ (E ∨ F) ∨ B ∨ A ∨ (E ∨ F) → A ∨ B ∨ (E ∨ F) :=
    by intro h
       factor h, 5
\end{minted}

Here, the tactic was used to eliminate the second occurrences of \texttt{A}, \texttt{B} and
\texttt{(E ∨ F)} at the hypothesis \texttt{h}.

\section{Linear Arithmetic Reasoning}

\subsection*{Summing Lists of Inequalities\\Rule Statement:}
\[
  \infer[]{\sum_{i = 1}^{n} a_{i} \bowtie^{*} \sum_{i = 1}^{n} b_{i}}{\bigwedge_{i = 1}^{n} a_{i} \bowtie_{i} b_{i}}
\]

Given a list of $n$ proofs of statements following one of the patterns $a_{i} < b_{i}$, $a_{i} \le b_{i}$ or $a_{i} = b_{i}$, this
tactic produces a proof of the inequality $\sum_{i = 1}^{n} a_{i} \bowtie^{*} \sum_{i = 1}^{n} b_{i}$, where $\bowtie^{*}$ is $<$ if
all terms in the premisses are strict inequalities and $\le$ otherwise.

Each one of the variables $a_{i}$ and $b_{i}$ might be represented inside the SMT solver
with either the \textit{Int} or the \textit{Real} sort. While each pair $a_{i}$ and $b_{i}$ always share the same sort, it is possible that some of these pairs have a sort different from the rest. Before implementing a
tactic corresponding to this rule, we have to choose types in Lean to match
these two sorts. We have based our decision on the types used by mathlib, as
individuals formalizing new mathematical statements in this library are one of the main potential users
of a Lean hammer. Integers are represented in mathlib exclusively with the built-in type \textit{Int}, so we will use it to represent the Int sort.
For the Real sort, we could either represent it using the type \texttt{Real}, which was defined in mathlib
to represent real numbers, or the type \texttt{Rat}, which was defined in the package \textit{std4}\footnote{std4 is Lean's standard library. It can be accessed at: \url{https://github.com/leanprover/std4}} to
represent rational numbers and is used by mathlib to formalize a variety of theorems
that involve this type of number.
It is not a problem to use rational numbers to represent the Real sort since the rules employed by cvc5 in the theory of linear arithmetic
do not explore any particular property that is enjoyed by real numbers and not by
rational numbers.
We decided to employ the type \texttt{Rat}, as its definition is much simpler and easier
to manage in comparison to the one for \texttt{Real}.

The implementation of this tactic will require 9 variations of the following theorem:

\begin{minted}{lean}
  sumBounds {α : Type} [LinearOrderedRing α] {a b c d : α} :
    a < b → c < d → a + c < b + d
\end{minted}

Each variation will correspond to one combination of the relation symbols in the hypothesis, where
they can be either $<$, $\le$ or $=$. The relation symbol in the
conclusion is adapted accordingly in each theorem. Since the rule accepts mixing of variables
from Int and Real sort, we need a variation of each one of those 9 theorems for each combination
of the types of the variables. Instead of stating all the combinations explicitly, which would
result in a total of 36 theorems and a long branch in the implementation of the core functionality of the tactic,
we stated only one polymorphic version of each, as indicated by the type parameter \texttt{α} in \texttt{sumBounds}.
Obviously, the theorem does not hold
for any \texttt{α} (it cannot even be stated if there is no comparison and addition operators defined over \texttt{α}).
We solve this issue by adding a restriction, stating that \texttt{α} satisfies the axioms of a
\textit{Linear Ordered Ring} (a class of types that contains both \texttt{Int} and \texttt{Rat} defined in mathlib), represented by the expression \texttt{[LinearOrderedRing α]} in the theorem. With this restriction we
could find a proof for each theorem.

\begin{figure}[t]
\begin{minted}[linenos]{lean}
  def combineBounds (pf₁ pf₂ : Expr) : MetaM Expr := do
    let t₁ ← inferType pf₁
    let t₂ ← inferType pf₂
    let rel₁ ← getRel t₁
    let rel₂ ← getRel t₂
    let tp₁ ← getOperandType t₁
    let tp₂ ← getOperandType t₂
    let (pf₁, pf₂) ← castHypothesis pf₁ pf₂ rel₁ rel₂ tp₁ tp₂
    let thmName : Name :=
      match rel₁, rel₂ with
      | `LT.lt , `LT.lt => `sumBounds₁
      | `LT.lt , `LE.le => `sumBounds₂
      | _      , _      => panic! "[sumBounds]: invalid relation"
    mkAppM thmName #[pf₂, pf₁]

  def sumBoundsCore (acc : Expr) (pfs : List Expr) : MetaM Expr :=
    match pfs with
    | [] => return acc
    | pf :: pfs' => do
      let acc' ← combineBounds acc pf'
      sumBoundsCore acc' pfs'
\end{minted}
\caption{Implementation of the SumBounds tactic}
\end{figure}

To implement this tactic, we apply the theorems for summing bounds in each element
of the list of proofs received, accumulating the results. We start the accumulator
with the last

After proving the necessary theorems, we can implement the tactic in the following manner:
we process the proofs received as arguments in reverse order, maintaining a variable \texttt{pf}
that accumulates the combined result of all the proofs processed so far, which is initialized
with the last proof in our list. For each proof we go through, we obtain the statement it corresponds to
with \texttt{inferType} and analyze its relation symbol and the relation symbol associated with the
current type of \texttt{pf}. With these two informations we decide which of the variants of the theorem
\texttt{sumBounds} we can apply in this two proofs. Also, we need to check what are the types of the
variables in these proofs to properly instantiate \texttt{α}. If one of them is \texttt{Rat} and the
other is \texttt{Int}, we also have to cast the inequality between integers into
an inequality between the same terms, casted to rationals, since the \texttt{sumBounds} theorems expect all variables to have the same type. This is done using one of the
following three theorems, depending on the relation symbol:

\begin{itemize}
  \item \mintinline{lean}{Int.castEQ : ∀ {a b : Int}, a < b → Rat.ofInt a < Rat.ofInt b}
  \item \mintinline{lean}{Int.castLE : ∀ {a b : Int}, a ≤ b → Rat.ofInt a ≤ Rat.ofInt b}
  \item \mintinline{lean}{Int.castLT : ∀ {a b : Int}, a = b → Rat.ofInt a = Rat.ofInt b}
\end{itemize}

where \texttt{Rat.ofInt} is the standard function to cast an integer into a rational.

\subsection*{MulPosNeg\\Rule Statement:}
% \[
%   \infer[]{}{}
% \]

% \subsection{TightBounds}


% TODO: put this in an appendix
\begin{table}[]\label{tab:rules}
\centering
\begin{tabular}{ l l l }
\toprule
Name        & Statement & Implementation \\ \midrule
NotImplies1 & \texttt{Not (p -> q) -> p}      & theorem        \\ \midrule
NotImplies2 & blah      & theorem        \\ \midrule
EquivElim1  & blah      & theorem        \\ \midrule
EquivElim2  & blah      & theorem        \\ \midrule
NotEquivElim1  & blah      & theorem        \\ \midrule
NotEquivElim2  & blah      & theorem        \\ \midrule
ImpliesElim & blah      & theorem        \\ \bottomrule
\end{tabular}
\caption{Rules from cvc5's proof calculus}
\end{table}


% proof of $P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n} \rightarrow (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$, together with \texttt{congOrLeft} to surpass
% the propositions to the left of $P_{i - 1}$. We need one application of \texttt{congOrLeft}
% for each proposition to the left of $P_{i - 1}$. For instance, if:
% \begin{itemize}
%   \item \textbf{i = 2} our term will be \texttt{orAssoc}
%   \item \textbf{i = 3} our term will be \texttt{congOrLeft orAssoc}
%   \item \textbf{i = 4} our term will be \texttt{congOrLeft (congOrLeft orAssoc)}
% \end{itemize}


% The idea is to use \texttt{congOrLeft} to surpass the propositions to the left of $P_{i - 1}$ together with \texttt{orAssoc} instantiating \texttt{A} to $P_{i - 1}$, \texttt{B} to $P_{i}$ and \texttt{C} to $P_{i + 1} \vee \cdots \vee
% P_{n}$, which will yield a proof of $P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n} \rightarrow (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.


% We cannot directly apply it to $h$ since there are potentially other terms to the left of $P_{i - 1}$.
% To solve this problem, we use the theorem \texttt{congOrLeft} setting \texttt{hyp} to the application of \texttt{orAssoc} that we just described.
% Since the first $i - 2$ propositions are not grouped, we cannot instantiate \texttt{C} to $P_{i - 1} \vee \cdots \vee P_{i - 2}$.
% Instead, we set \texttt{C} to $P_{i - 2}$, which will yield a proof \texttt{h₂} of $P_{i - 2} \vee P_{i - 1} \vee P_{i} \vee \cdots \vee P_{n} \rightarrow P_{i - 2} \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$. Now we repeat the process applying \texttt{congOrLeft} with \texttt{C} instantiated to $P_{i - 3}$ (if it exists) and \texttt{hyp} to \texttt{h₂}. We keep going until we set \texttt{C} to $P_{1}$. We will then obtain a term of the form \texttt{congOrLeft (congOrLeft \ldots (congOrLeft orAssoc)\ldots)} (with $i - 2$ applications of \texttt{congOrLeft}), which, when applied to \texttt{h}, will prove $P_{1} \vee \cdots \vee (P_{i - 1} \vee P_{i}) \vee \cdots \vee P_{n}$.


% One detail that has to be considered while implementing this tactic is that the index of the last proposition
% may change when we pull a term. For instance, the index of the last proposition of the clause
% $A \vee B \vee (C \vee D) \vee E$ is $4$. If we pull $E$ in this clause, we will get
% $E \vee A \vee B \vee (C \vee D)$, which has $3$ as this index. If this happens,
% the new index will always be the original length of the clause minus the length
% of the second to last proposition in the current clause (since it will become the last proposition).
% While executing the procedure we described in the last paragraph, we need to always keep track of the
% current index of the last proposition, as the tactic pull requires it as an argument.

% \begin{figure}[t]
% % \textbf{Input:} $\psi$, a PL formula\\
% % \textbf{Output:} \textit{true} or \textit{false}, depending whether $\psi$ is satisfiable
% \begin{algorithmic}[1]
% \Function{PermutateClauseCore}{$pf$, $perm$, $s$}
%   \State $clause \gets $ \Call{InferType}{$pf$}
%   \State $lenClause \gets $ \Call{getLength}{$clause$}
%   \For{$i \gets n \Downto 1$}
%   \If{$perm_{i} = s$}
%     \State $currentClause \gets$ \Call{InferType}{pf}
%     \State $lenSecondLast \gets $ \Call{GetLength}{$currentClause_{s - 1}$}
%     \State $pf \gets$ \Call{PullCore}{$pf$, $perm_{i}$, $s$}
%     \State $s \gets lenSecondLast - lenClause$
%   \Else
%     \State $pf \gets$ \Call{PullCore}{$pf$, $perm_{i}$, $s$}
%   \EndIf
%   \EndFor
%   \State \Return~$pf$
% \EndFunction
% \end{algorithmic}
% \caption{Implementation of the tactic PermutateClause}~\label{permClause}
% \end{figure}
