\label{sec:hammering}
As previously mentioned, Hammering Towards QED is a project
that aims to describe all the tools, which the paper calls ``hammers'',
that were created with the purpose of connecting automatic and interactive
theorem provers. Besides that, this document also outlines
the main components that such tools usually have. They are
the following:

\begin{itemize}
  \item The premiss selection module: that identifies
        a subset of the facts previously demonstrated in the
        ITP that are more likely to be useful in order to
        prove the given goal, to be dispatched to the ATP.\@
  \item The translation module: that builds a problem in the language of the
        ATP that corresponds to the original goal from the ITP and using
        the premisses that were selected.\@
  \item The proof reconstruction module: that lifts the proof produced
        by the ATP into a proof that is accepted by the ITP.\@
\end{itemize}

In our case, we will restrict ourselves to implement a proof reconstruction module.
The three main strategies used to reconstruct the proof produced
by the automatic system inside the interactive one are also described in the paper.

The first one is to use the ITP to verify a deeply embedded version of the proof
received, and, in case it is succesful, reflect this proof inside it's checker, proving
the original goal. More specifically, the hammer defines a datatype to represent
terms in the ATP and a set of functions to manipulate values on those datatypes, representing
the axioms that the solver uses to transform the terms. Then, a lifting function is defined, that is,
a function that take a value of this datatype and outputs an equivalent term in the native
language of the ITP.\ Finally, the correctness of each transformation function is
verified with respect to the lifting function, in the sense that, if the input term
was lifted to a value that is provable in the ITP's logic, then the output term will
also be provable. The ATP's proof will be represented as a sequence of those
transformations, and their correctness are proved \textit{a priori}. When checking
a specific proof, the only step that the ITP must perform is to compute the result
of the application of all the functions in the solver's output, and to check if
the final term matches with the expected one. This technique is known as
the Certified approach\ \cite{snipe}.

The second one is to match each axiom in the ATP's logic into a proved lemma or a tactic
\tom{is it okay to use tactic here? should I introduce the term first?}
defined in the ITP that works directly with native terms of the system.
The proof produced by the automatic solver is then parsed into a sequence of applications
of those lemmas and tactics and replayed inside the ITP.\ In this case, the proof is built
on the fly and doesn't have it's correctness guaranteed (it can fail in the middle of the
proccess in case the ATP or the hammer did something wrong). On the other hand,
this technique skips computations done over embedded terms, which have to be done by the
Certified approach, having the potential to have a better
performance. This approach is known as the Certifying approach\ \cite{snipe}.

The third one is to compile the proof into the ITP's source code. This implies generating
an actual script in the native language of the interactive system
that corresponds to the proof received. After the script is generated, it is
possible to postprocess\tom{maybe copy the references that hammering towards use to talk about this?}
it in order to make the proof easier to be checked,
in a way that the final script can possibly ignore a large portion of the
original proof. This approach can be inconvenient for very large proofs,
as it requires that the script is stored in some filesystem. However,
it has the advantage over the two previous methods of only requiring access to the ATP
on the first time that the proof is checked.

In this project we will be using the second approach. We give more details about this
decision in the later chapters.
